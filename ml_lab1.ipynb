{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1. Линейная регрессия. Нормальное уравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если для оценки качества в регресии используется среднеквадратичная ошибка (*mean squared error, MSE*), то ошибка на одном примере (*функция потерь*) будет определяться выражением:\n",
    "\n",
    "$$L(y,a)=(a-y)^2$$\n",
    "\n",
    "а суммарная ошибка (*функционал ошибки*):\n",
    "\n",
    "$$MSE(a,X)=\\frac1{l}\\sum_{i=1}^lL(y_i,a(\\overrightarrow{x_i}))=\\frac1{l}\\sum_{i=1}^l(a(\\overrightarrow{x_i})-y_i)^2$$\n",
    "\n",
    "В случае линейной регресии:\n",
    "\n",
    "$$a(\\overrightarrow{x_i})=\\langle \\overrightarrow{w},\\overrightarrow{x_i}\\rangle$$\n",
    "\n",
    "Задача оптимизации:\n",
    "\n",
    "$$\\frac1{l}\\sum_{i=1}^l (\\langle \\overrightarrow{w},\\overrightarrow{x_i}\\rangle-y_i)^2\\to \\min_{\\overrightarrow{w}}$$\n",
    "\n",
    "Тогда, продифференцировав функционал ошибки по $\\overrightarrow{w}$, приравняв его нулю и решив полученное уравнение, получим следующее выражение для оптимального вектора весов, которое называется *нормальным уравнением*:\n",
    "\n",
    "$$\\overrightarrow{w}_{opt} = \\left(X^TX\\right)^{-1}X^T\\overrightarrow{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Давайте разберемся с этими формулами шаг за шагом.\n",
    "\n",
    "### 1. Функция потерь для одного примера:\n",
    "\n",
    "В регрессии мы часто используем среднеквадратичную ошибку (*mean squared error, MSE*) в качестве метрики качества. Для одного примера эта ошибка, или *функция потерь*, определяется как:\n",
    "\n",
    "$$L(y, a) = (a - y)^2,$$\n",
    "\n",
    "где:\n",
    "- \\(L\\) - функция потерь,\n",
    "- \\(y\\) - истинное значение целевой переменной,\n",
    "- \\(a\\) - предсказанное значение целевой переменной.\n",
    "\n",
    "### 2. Суммарная ошибка (Функционал ошибки):\n",
    "\n",
    "Суммарная ошибка, или *функционал ошибки*, для всего набора данных определяется как среднее значение функции потерь:\n",
    "\n",
    "$$MSE(a, X) = \\frac{1}{l} \\sum_{i=1}^{l} L(y_i, a(\\overrightarrow{x_i})) = \\frac{1}{l} \\sum_{i=1}^{l} (a(\\overrightarrow{x_i}) - y_i)^2,$$\n",
    "\n",
    "где:\n",
    "- \\(MSE\\) - функционал ошибки,\n",
    "- \\(l\\) - количество примеров в наборе данных,\n",
    "- \\(y_i\\) - истинное значение целевой переменной для \\(i\\)-го примера,\n",
    "- \\(a(\\overrightarrow{x_i})\\) - предсказанное значение целевой переменной для \\(i\\)-го примера.\n",
    "\n",
    "### 3. Линейная регрессия:\n",
    "\n",
    "В случае линейной регрессии предсказанное значение \\(a(\\overrightarrow{x_i})\\) выражается как скалярное произведение весового вектора \\(\\overrightarrow{w}\\) и вектора признаков \\(\\overrightarrow{x_i}\\):\n",
    "\n",
    "$$a(\\overrightarrow{x_i}) = \\langle \\overrightarrow{w}, \\overrightarrow{x_i} \\rangle.$$\n",
    "\n",
    "### 4. Задача оптимизации:\n",
    "\n",
    "Цель линейной регрессии - минимизировать суммарную ошибку. Формально задача оптимизации выглядит следующим образом:\n",
    "\n",
    "$$\\frac{1}{l}\\sum_{i=1}^{l} (\\langle \\overrightarrow{w}, \\overrightarrow{x_i} \\rangle - y_i)^2 \\to \\min_{\\overrightarrow{w}}$$\n",
    "\n",
    "где:\n",
    "- \\(\\overrightarrow{w}\\) - вектор весов, который мы хотим найти.\n",
    "\n",
    "### 5. Нормальное уравнение:\n",
    "\n",
    "Решением этой задачи оптимизации является вектор весов, который можно найти, продифференцировав функционал ошибки по весам, приравняв его к нулю и решив уравнение. Это приводит к *нормальному уравнению*:\n",
    "\n",
    "$$\\overrightarrow{w}_{opt} = \\left(X^TX\\right)^{-1}X^T\\overrightarrow{y}.$$\n",
    "\n",
    "где:\n",
    "- \\(\\overrightarrow{w}_{opt}\\) - оптимальный вектор весов,\n",
    "- \\(X\\) - матрица признаков,\n",
    "- \\(\\overrightarrow{y}\\) - вектор целевых значений.\n",
    "\n",
    "Это уравнение предоставляет аналитическое решение для оптимальных весов в задаче линейной регрессии. Однако, как упоминалось ранее, обращение матрицы \\(X^TX\\) может быть трудозатратным в случае большого количества признаков, и поэтому градиентные методы (градиентный спуск) предпочтительны в практических задачах."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1. Пример из лекций**\n",
    "\n",
    "Напишите функцию ``get_weight``, которая находит вектор весов на основе нормального уравнения.\n",
    "\n",
    "Полезные функции: ``numpy.ones(n)`` для создания массива из единиц длины $n$ и ``numpy.concatenate((А, В), axis=1)`` для слияния двух матриц по столбцам (пара ``А`` и ``В`` превращается в матрицу ``[A B]``).\n",
    "\n",
    "Проверьте работу функции на простом примере из лекций:\n",
    "\n",
    "$$x_1=2, x_2=3, x_3=5$$\n",
    "\n",
    "$$y_1=1, y_2=3, y_3=4$$\n",
    "\n",
    "Имейте в виду, что $X$ – это матрица (в данном примере состоящая из одного столбца).\n",
    "\n",
    "Нарисуйте исходные данные и полученную линию регресии при помощи ``matplotlib``: для рисования точек используйте ``plt.scatter``, для рисования линии – ``plt.plot``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "X = np.array([[2], [3], [5]])\n",
    "y = np.array([1, 3, 4])\n",
    "\n",
    "# Добавляем столбец из единиц\n",
    "X_with_bias = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "# Получаем вектор весов\n",
    "weights = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n",
    "\n",
    "# Прогнозные значения\n",
    "y_pred = X_with_bias @ weights\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Вектор весов:\", weights)\n",
    "print(\"Прогнозные значения:\", y_pred)\n",
    "\n",
    "# Вычисляем функционалы ошибки\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = 1 - (np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2))\n",
    "\n",
    "# Выводим значения функционалов ошибки\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R²):\", r2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T12:32:08.348124900Z",
     "start_time": "2023-11-17T12:32:07.413629200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 24\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m weights\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Пример использования функции\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Предположим, у нас есть данные X и y\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m X \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m], [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m], [\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m6\u001B[39m]])\n\u001B[0;32m     25\u001B[0m y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m11\u001B[39m])\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Получаем вектор весов\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def get_weight(X, y):\n",
    "    \"\"\"\n",
    "    Находит вектор весов на основе нормального уравнения.\n",
    "\n",
    "    Параметры:\n",
    "    - X: Матрица признаков (numpy array), где каждая строка представляет собой один пример,\n",
    "         а каждый столбец представляет собой один признак.\n",
    "    - y: Вектор целевых значений (numpy array).\n",
    "\n",
    "    Возвращает:\n",
    "    - Вектор весов (numpy array), который минимизирует сумму квадратов ошибок.\n",
    "    \"\"\"\n",
    "\n",
    "    # Добавляем столбец из единиц к матрице X для учета свободного члена в уравнении\n",
    "    X_with_bias = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Нормальное уравнение: w = (X^T * X)^(-1) * X^T * y\n",
    "    weights = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Пример использования функции\n",
    "# Предположим, у нас есть данные X и y\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([3, 7, 11])\n",
    "\n",
    "# Получаем вектор весов\n",
    "weights = get_weight(X, y)\n",
    "\n",
    "print(\"Вектор весов:\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_line(X, y, weights):\n",
    "    \"\"\"\n",
    "    Нарисовать исходные данные и линию регрессии.\n",
    "\n",
    "    Параметры:\n",
    "    - X: Матрица признаков (numpy array).\n",
    "    - y: Вектор целевых значений (numpy array).\n",
    "    - weights: Вектор весов (numpy array).\n",
    "    \"\"\"\n",
    "\n",
    "    # Рисуем точки данных\n",
    "    plt.scatter(X[:, 1], y, color='blue', label='Исходные данные')\n",
    "\n",
    "    # Рисуем линию регрессии\n",
    "    x_line = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n",
    "    y_line = weights[0] + weights[1] * x_line\n",
    "    plt.plot(x_line, y_line, color='red', label='Линия регрессии')\n",
    "\n",
    "    # Добавляем подписи и легенду\n",
    "    plt.xlabel('Признак')\n",
    "    plt.ylabel('Целевое значение')\n",
    "    plt.title('Линия регрессии')\n",
    "    plt.legend()\n",
    "\n",
    "    # Показываем график\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования функции с предыдущими данными\n",
    "plot_regression_line(X, y, weights)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите значения функционалов ошибки $MSE$, $RMSE$, $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_test, y_predict):\n",
    "    return np.mean((y_test - y_predict)**2)\n",
    "    # Ваш код здесь\n",
    "    \n",
    "\n",
    "def RMSE(y_test, y_predict):\n",
    "    return np.sqrt(MSE(y_test, y_predict))\n",
    "\n",
    "\n",
    "def R2(y_test, y_predict):\n",
    "    y_mean = np.mean(y_test)\n",
    "    ss_total = np.sum((y_test - y_mean)**2)\n",
    "    ss_residual = np.sum((y_test - y_predict)**2)\n",
    "    return 1 - (ss_residual / ss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните полученные значения с библиотечными функциями $MSE$ и $R2$ из [scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Прогнозные значения\n",
    "y_pred = X_with_bias @ weights\n",
    "\n",
    "# Вычисление функционалов ошибкиОператор\n",
    "# @ в выражении y_pred = X_with_bias @ weights используется для умножения матрицы X_with_bias на вектор весов weights, чтобы получить прогнозные значения целевой переменной.\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = root_mean_squared_error(y, y_pred)\n",
    "r2 = r_squared(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R²):\", r2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2. Более сложный пример**.\n",
    "Скачайте файлы ``ml_lab1_train.txt`` и ``ml_lab1_test.txt``. В первом из них находится обучающая выборка, а во втором – тестовая. Каждый из файлов содержит два столбца чисел, разделённых пробелами: в первом – $n$ точек (значения аргумента $x$), во втором – значения некоторой функции $y = f(x)$ в этих точках, искажённые случайным шумом. Ваша задача – по обучающей выборке подобрать функцию $y = a(x)$, приближающую неизвестную вам зависимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим обучающие и тестовые данные (не забудьте ввести правильный путь!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.loadtxt('.../ml_lab1_train.txt', delimiter=',')\n",
    "data_test = np.loadtxt('.../ml_lab1_test.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T12:59:12.662713700Z",
     "start_time": "2023-11-17T12:59:12.635790700Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train \u001B[38;5;241m=\u001B[39m data_train[:,\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      2\u001B[0m y_train \u001B[38;5;241m=\u001B[39m data_train[:,\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = data_train[:,0]\n",
    "y_train = data_train[:,1]\n",
    "\n",
    "# Сделайте то же для тестовой выборки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите с помощью функции ``get_weight`` линейную функцию ($y = kx + b$), наилучшим образом приближающую неизвестную зависимость.  \n",
    "Выведите значения весовых коэффициентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "# Загрузка данных из файла\n",
    "train_data = np.loadtxt(\"ml_lab1_train.txt\", delimiter=',')\n",
    "X_train = train_data[:, 0].reshape(-1, 1)\n",
    "y_train = train_data[:, 1]\n",
    "\n",
    "# Получаем веса для линейной регрессии (y = kx + b)\n",
    "weights = get_weight(X_train, y_train)\n",
    "\n",
    "# Выводим значения весовых коэффициентов\n",
    "k, b = weights[:-1]  # Последний элемент - свободный член\n",
    "print(\"Значение наклона (k):\", k)\n",
    "print(\"Значение свободного члена (b):\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте на плоскости точки обучающей и тестовой выборок (раскрасив в два цвета) $(x_i, y_i)$ и полученную линейную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_linear_regression(X_train, y_train, X_test, y_test, weights):\n",
    "    \"\"\"\n",
    "    Нарисовать точки обучающей и тестовой выборок и линейную регрессию.\n",
    "\n",
    "    Параметры:\n",
    "    - X_train: Матрица признаков обучающей выборки.\n",
    "    - y_train: Вектор целевых значений обучающей выборки.\n",
    "    - X_test: Матрица признаков тестовой выборки.\n",
    "    - y_test: Вектор целевых значений тестовой выборки.\n",
    "    - weights: Вектор весов линейной регрессии.\n",
    "    \"\"\"\n",
    "\n",
    "    # Обучающая выборка\n",
    "    plt.scatter(X_train, y_train, color='blue', label='Обучающая выборка')\n",
    "\n",
    "    # Тестовая выборка\n",
    "    plt.scatter(X_test, y_test, color='red', label='Тестовая выборка')\n",
    "\n",
    "    # Линейная регрессия\n",
    "    x_line = np.linspace(np.min(np.concatenate((X_train, X_test))), np.max(np.concatenate((X_train, X_test))), 100)\n",
    "    y_line = weights[0] * x_line + weights[1]\n",
    "    plt.plot(x_line, y_line, color='green', label='Линейная регрессия')\n",
    "\n",
    "    # Добавляем подписи и легенду\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Линейная регрессия и выборки')\n",
    "    plt.legend()\n",
    "\n",
    "    # Показываем график\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования с вашими данными\n",
    "# Загрузка данных из файла\n",
    "train_data = np.loadtxt(\"ml_lab1_train.txt\", delimiter=',')\n",
    "test_data = np.loadtxt(\"ml_lab1_test.txt\", delimiter=',')\n",
    "\n",
    "X_train = train_data[:, 0].reshape(-1, 1)\n",
    "y_train = train_data[:, 1]\n",
    "\n",
    "X_test = test_data[:, 0].reshape(-1, 1)\n",
    "y_test = test_data[:, 1]\n",
    "\n",
    "# Получаем веса для линейной регрессии (y = kx + b)\n",
    "weights = get_weight(X_train, y_train)\n",
    "\n",
    "# Рисуем график\n",
    "plot_linear_regression(X_train, y_train, X_test, y_test, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите значения функционалов ошибки $MSE$, $RMSE$, $R^2$. Сравните их со значениями библиотечных функций `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Рассчитывает значения функционалов ошибки MSE, RMSE, R².\n",
    "\n",
    "    Параметры:\n",
    "    - y_true: Фактические значения.\n",
    "    - y_pred: Прогнозные значения.\n",
    "\n",
    "    Возвращает:\n",
    "    - Значения MSE, RMSE, R².\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, r2\n",
    "\n",
    "# Пример использования с вашими данными\n",
    "# Загрузка данных из файла\n",
    "train_data = np.loadtxt(\"ml_lab1_train.txt\", delimiter=',')\n",
    "test_data = np.loadtxt(\"ml_lab1_test.txt\", delimiter=',')\n",
    "\n",
    "X_train = train_data[:, 0].reshape(-1, 1)\n",
    "y_train = train_data[:, 1]\n",
    "\n",
    "X_test = test_data[:, 0].reshape(-1, 1)\n",
    "y_test = test_data[:, 1]\n",
    "\n",
    "# Получаем веса для линейной регрессии (y = kx + b)\n",
    "weights = get_weight(X_train, y_train)\n",
    "\n",
    "# Прогнозные значения\n",
    "y_train_pred = X_train @ weights[:-1] + weights[-1]\n",
    "y_test_pred = X_test @ weights[:-1] + weights[-1]\n",
    "\n",
    "# Рассчитываем метрики ошибки\n",
    "mse_train, rmse_train, r2_train = calculate_metrics(y_train, y_train_pred)\n",
    "mse_test, rmse_test, r2_test = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Обучающая выборка:\")\n",
    "print(\"MSE: \", mse_train)\n",
    "print(\"RMSE:\", rmse_train)\n",
    "print(\"R²:  \", r2_train)\n",
    "print(\"\\nТестовая выборка:\")\n",
    "print(\"MSE: \", mse_test)\n",
    "print(\"RMSE:\", rmse_test)\n",
    "print(\"R²:  \", r2_test)\n",
    "\n",
    "# Теперь используем LinearRegression из scikit-learn для сравнения\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred_sklearn = model.predict(X_train)\n",
    "y_test_pred_sklearn = model.predict(X_test)\n",
    "\n",
    "mse_train_sklearn, rmse_train_sklearn, r2_train_sklearn = calculate_metrics(y_train, y_train_pred_sklearn)\n",
    "mse_test_sklearn, rmse_test_sklearn, r2_test_sklearn = calculate_metrics(y_test, y_test_pred_sklearn)\n",
    "\n",
    "# Выводим результаты scikit-learn\n",
    "print(\"\\nРезультаты scikit-learn (LinearRegression):\")\n",
    "print(\"Обучающая выборка:\")\n",
    "print(\"MSE: \", mse_train_sklearn)\n",
    "print(\"RMSE:\", rmse_train_sklearn)\n",
    "print(\"R²:  \", r2_train_sklearn)\n",
    "print(\"\\nТестовая выборка:\")\n",
    "print(\"MSE: \", mse_test_sklearn)\n",
    "print(\"RMSE:\", rmse_test_sklearn)\n",
    "print(\"R²:  \", r2_test_sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
