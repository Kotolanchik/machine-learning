{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2. Линейная регрессия. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним нормальное уравнение:\n",
    "\n",
    "$$\\overrightarrow{w}_{opt} = \\left(X^TX\\right)^{-1}X^T\\overrightarrow{y}.$$\n",
    "\n",
    "Здесь присутствует обращение матрицы $X^TX$ – довольно трудоёмкая операция при большом количестве признаков: сложность вычислений $O(d^3)$. При решении реальных задач такая трудоёмкость часто оказывается непозволительной, поэтому параметры модели (весовые коэффициенты) ищут итерационными методами, стоимость которых меньше. Один из них – *градиентный спуск* (gradient descent – ['greɪdɪənt dɪ'sent]).\n",
    "\n",
    "Напомним, что в градиентном спуске значения параметров на следующем шаге получаются из значений параметров на текущем шаге смещением в сторону антиградиента функционала ошибки: \n",
    "\n",
    "$$\\overrightarrow{w}^{(k+1)} = \\overrightarrow{w}^{(k)} - \\eta_k \\nabla Q(\\overrightarrow{w}^{(k)}),$$\n",
    "где $\\eta_k$ – шаг градиентного спуска.\n",
    "\n",
    "Формула градиента функционала ошибки выглядит следующим образом:\n",
    "\n",
    "$$\\nabla Q(\\overrightarrow{w}) = \\nabla_\\overrightarrow{w}\\left(\\frac{1}{l}\\|X\\overrightarrow{w}-\\overrightarrow{y}\\|^2\\right) = \\frac{2}{l}X^T(X\\overrightarrow{w} - \\overrightarrow{y}).$$\n",
    " \n",
    "Сложность вычислений в данном случае $O(dl)$."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Давайте разберемся с этими формулами пошагово.\n",
    "\n",
    "### 1. Формула оптимальных весов:\n",
    "\n",
    "$$\\overrightarrow{w}_{opt} = \\left(X^TX\\right)^{-1}X^T\\overrightarrow{y}.$$\n",
    "\n",
    "Эта формула представляет собой решение нормального уравнения для задачи линейной регрессии. Здесь:\n",
    "\n",
    "- \\(X\\) - матрица признаков (каждая строка представляет собой один пример, каждый столбец - один признак),\n",
    "- \\(\\overrightarrow{y}\\) - вектор целевых значений,\n",
    "- \\(\\overrightarrow{w}_{opt}\\) - вектор оптимальных весов.\n",
    "\n",
    "Обращение матрицы \\(X^TX\\) - это обратная операция, и ее сложность \\(O(d^3)\\), где \\(d\\) - количество признаков. Это может быть трудозатратной операцией при большом числе признаков.\n",
    "\n",
    "### 2. Градиентный спуск:\n",
    "\n",
    "Вместо использования нормального уравнения, который может быть трудозатратным в случае большого числа признаков, мы можем использовать метод градиентного спуска для нахождения оптимальных весов. \n",
    "\n",
    "Формула обновления весов на каждом шаге градиентного спуска:\n",
    "\n",
    "$$\\overrightarrow{w}^{(k+1)} = \\overrightarrow{w}^{(k)} - \\eta_k \\nabla Q(\\overrightarrow{w}^{(k)}).$$\n",
    "\n",
    "Здесь:\n",
    "\n",
    "- \\(\\overrightarrow{w}^{(k)}\\) - вектор весов на \\(k\\)-том шаге,\n",
    "- \\(\\eta_k\\) - шаг градиентного спуска,\n",
    "- \\(\\nabla Q(\\overrightarrow{w})\\) - градиент функционала ошибки.\n",
    "\n",
    "### 3. Градиент функционала ошибки:\n",
    "\n",
    "Формула градиента функционала ошибки в случае линейной регрессии:\n",
    "\n",
    "$$\\nabla Q(\\overrightarrow{w}) = \\frac{2}{l}X^T(X\\overrightarrow{w} - \\overrightarrow{y}).$$\n",
    "\n",
    "Здесь:\n",
    "\n",
    "- \\(l\\) - количество примеров в обучающей выборке.\n",
    "\n",
    "Сложность вычислений этого градиента \\(O(dl)\\), что может быть более эффективным для больших наборов данных и признаков, чем обращение матрицы \\(X^TX\\).\n",
    "\n",
    "Градиентный спуск и его вариации (стохастический градиентный спуск, градиентный спуск по мини-батчам) используются в практике машинного обучения для эффективного нахождения оптимальных весов. Они особенно полезны, когда обращение матрицы становится слишком сложным или невозможным."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1. Реализация градиентного спуска**  \n",
    "\n",
    "Напишите функцию `gradient_descent`, которая находит вектор весов на основе градиентного спуска.  \n",
    "\n",
    "В качестве критериев остановки можно использовать максимальное количество шагов и/или количество шагов, при котором отсутствуют значимые изменения весов.\n",
    "\n",
    "Проверьте работу функции на простом примере из лекций:\n",
    "\n",
    "$$x_1=2, x_2=3, x_3=5,$$\n",
    "\n",
    "$$y_1=1, y_2=3, y_3=4.$$\n",
    "\n",
    "Нарисуйте исходные данные и полученную линию регресии при помощи ``matplotlib``: для рисования точек используйте ``plt.scatter``, для рисования линии – ``plt.plot``.  \n",
    "\n",
    "Сравните полученные результаты с результатами, полученными на основе нормального уравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.01, max_iter=1000, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Находит вектор весов с использованием градиентного спуска.\n",
    "\n",
    "    Параметры:\n",
    "    - X: Матрица признаков.\n",
    "    - y: Вектор целевых значений.\n",
    "    - learning_rate: Скорость обучения.\n",
    "    - max_iter: Максимальное количество итераций.\n",
    "    - tol: Критерий остановки по изменению весов.\n",
    "\n",
    "    Возвращает:\n",
    "    - Вектор весов.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    prev_weights = np.zeros(n)\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < max_iter and np.linalg.norm(weights - prev_weights) > tol:\n",
    "        prev_weights = weights.copy()\n",
    "        gradient = -2/m * X.T @ (y - X @ weights)\n",
    "        weights -= learning_rate * gradient\n",
    "        iterations += 1\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[2], [3], [5]])\n",
    "y = np.array([1, 3, 4])\n",
    "\n",
    "# Добавляем столбец из единиц для учета свободного члена\n",
    "X_with_bias = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "# Получаем веса с использованием градиентного спуска\n",
    "weights_gradient_descent = gradient_descent(X_with_bias, y)\n",
    "\n",
    "# Получаем веса с использованием нормального уравнения (для сравнения)\n",
    "weights_normal_equation = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Веса (градиентный спуск):\", weights_gradient_descent)\n",
    "print(\"Веса (нормальное уравнение):\", weights_normal_equation)\n",
    "\n",
    "# Рисуем график исходных данных и линии регрессии\n",
    "plt.scatter(X, y, label='Исходные данные')\n",
    "x_line = np.linspace(np.min(X), np.max(X), 100).reshape(-1, 1)\n",
    "X_line_with_bias = np.concatenate((np.ones((x_line.shape[0], 1)), x_line), axis=1)\n",
    "\n",
    "y_line_gradient_descent = X_line_with_bias @ weights_gradient_descent\n",
    "y_line_normal_equation = X_line_with_bias @ weights_normal_equation\n",
    "\n",
    "plt.plot(x_line, y_line_gradient_descent, label='Линия регрессии (градиентный спуск)', color='red')\n",
    "plt.plot(x_line, y_line_normal_equation, label='Линия регрессии (нормальное уравнение)', linestyle='dashed', color='blue')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2. Исследование скорости спуска**  \n",
    "\n",
    "Протестируйте функцию `gradient_descent` на наборе данных `ml_lab1_train.txt`/`ml_lab1_test.txt` для разных значений скорости спуска $\\eta_k$, например, $\\eta_k = \\{0.5, 1.0, 2.0\\}$.  \n",
    "\n",
    "Оцените количество шагов для получения решения в каждом случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.01, max_iter=1000, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Находит вектор весов с использованием градиентного спуска.\n",
    "\n",
    "    Параметры:\n",
    "    - X: Матрица признаков.\n",
    "    - y: Вектор целевых значений.\n",
    "    - learning_rate: Скорость обучения.\n",
    "    - max_iter: Максимальное количество итераций.\n",
    "    - tol: Критерий остановки по изменению весов.\n",
    "\n",
    "    Возвращает:\n",
    "    - Вектор весов и количество итераций.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    prev_weights = np.zeros(n)\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < max_iter and np.linalg.norm(weights - prev_weights) > tol:\n",
    "        prev_weights = weights.copy()\n",
    "        gradient = -2/m * X.T @ (y - X @ weights)\n",
    "        weights -= learning_rate * gradient\n",
    "        iterations += 1\n",
    "\n",
    "    return weights, iterations\n",
    "\n",
    "# Загрузка данных из файла\n",
    "train_data = np.loadtxt(\"ml_lab1_train.txt\", delimiter=',')\n",
    "X_train = train_data[:, 0].reshape(-1, 1)\n",
    "y_train = train_data[:, 1]\n",
    "\n",
    "# Добавляем столбец из единиц для учета свободного члена\n",
    "X_train_with_bias = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "\n",
    "# Разные значения скорости обучения\n",
    "learning_rates = [0.5, 1.0, 2.0]\n",
    "\n",
    "# Протестируем для каждой скорости обучения\n",
    "for learning_rate in learning_rates:\n",
    "    weights, iterations = gradient_descent(X_train_with_bias, y_train, learning_rate=learning_rate)\n",
    "    print(f\"\\nСкорость обучения (η): {learning_rate}\")\n",
    "    print(\"Веса (градиентный спуск):\", weights)\n",
    "    print(\"Количество шагов:\", iterations)\n",
    "\n",
    "    # Рисуем график исходных данных и линии регрессии\n",
    "    plt.scatter(X_train, y_train, label='Исходные данные')\n",
    "    x_line = np.linspace(np.min(X_train), np.max(X_train), 100).reshape(-1, 1)\n",
    "    X_line_with_bias = np.concatenate((np.ones((x_line.shape[0], 1)), x_line), axis=1)\n",
    "\n",
    "    y_line_gradient_descent = X_line_with_bias @ weights\n",
    "    plt.plot(x_line, y_line_gradient_descent, label=f'Линия регрессии (η={learning_rate})', linestyle='dashed')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3. Стохастический градиентный спуск**  \n",
    "\n",
    "Стохастический градиентный спуск отличается от обычного заменой градиента на его оценку по одному или нескольким объектам. В этом случае сложность становится $O(kd)$, где $k$ – количество объектов, по которым оценивается градиент, $k<<l$. Это отчасти объясняет популярность стохастических методов оптимизации.  \n",
    "\n",
    "Реализуйте функцию `stochastic_gradient_descent`, которая находит вектор весов на основе стохастического градиентного спуска (вычисление градиента на одном случайном примере).  \n",
    "\n",
    "На наборе данных `ml_lab1_train.txt`/`ml_lab1_test.txt` оцените количество шагов для получения решения при разных значениях скорости спуска $\\eta_k$, например, $\\eta_k = \\{0.5, 1.0, 2.0\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, max_iter=1000, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Находит вектор весов с использованием стохастического градиентного спуска.\n",
    "\n",
    "    Параметры:\n",
    "    - X: Матрица признаков.\n",
    "    - y: Вектор целевых значений.\n",
    "    - learning_rate: Скорость обучения.\n",
    "    - max_iter: Максимальное количество итераций.\n",
    "    - tol: Критерий остановки по изменению весов.\n",
    "\n",
    "    Возвращает:\n",
    "    - Вектор весов и количество итераций.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    prev_weights = np.zeros(n)\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < max_iter and np.linalg.norm(weights - prev_weights) > tol:\n",
    "        prev_weights = weights.copy()\n",
    "        for i in range(m):\n",
    "            random_index = np.random.randint(0, m)\n",
    "            X_i = X[random_index, :].reshape(1, -1)\n",
    "            y_i = y[random_index]\n",
    "            gradient = -2 * X_i.T @ (y_i - X_i @ weights)\n",
    "            weights -= learning_rate * gradient.squeeze()\n",
    "        iterations += 1\n",
    "\n",
    "    return weights, iterations\n",
    "\n",
    "# Загрузка данных из файла\n",
    "train_data = np.loadtxt(\"ml_lab1_train.txt\", delimiter=',')\n",
    "X_train = train_data[:, 0].reshape(-1, 1)\n",
    "y_train = train_data[:, 1]\n",
    "\n",
    "# Добавляем столбец из единиц для учета свободного члена\n",
    "X_train_with_bias = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "\n",
    "# Разные значения скорости обучения\n",
    "learning_rates = [0.5, 1.0, 2.0]\n",
    "\n",
    "# Протестируем для каждой скорости обучения\n",
    "for learning_rate in learning_rates:\n",
    "    weights, iterations = stochastic_gradient_descent(X_train_with_bias, y_train, learning_rate=learning_rate)\n",
    "    print(f\"\\nСкорость обучения (η): {learning_rate}\")\n",
    "    print(\"Веса (стохастический градиентный спуск):\", weights)\n",
    "    print(\"Количество шагов:\", iterations)\n",
    "\n",
    "    # Рисуем график исходных данных и линии регрессии\n",
    "    plt.scatter(X_train, y_train, label='Исходные данные')\n",
    "    x_line = np.linspace(np.min(X_train), np.max(X_train), 100).reshape(-1, 1)\n",
    "    X_line_with_bias = np.concatenate((np.ones((x_line.shape[0], 1)), x_line), axis=1)\n",
    "\n",
    "    y_line_stochastic_gradient_descent = X_line_with_bias @ weights\n",
    "    plt.plot(x_line, y_line_stochastic_gradient_descent, label=f'Линия регрессии (η={learning_rate})', linestyle='dashed')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4. Градиентный спуск по мини-батчам**  \n",
    "\n",
    "Реализуйте функцию `mini_batch_gradient_descent`, которая находит вектор весов на основе градиентного спуска по мини-батчам (вычисление градиента на подмножестве случайно выбранных примеров). Размер мини-батча должен быть параметром функции.  \n",
    "\n",
    "На наборе данных `ml_lab1_train.txt`/`ml_lab1_test.txt` оцените количество шагов для получения решения при разных значениях скорости спуска $\\eta_k$, например, $\\eta_k = \\{0.5, 1.0, 2.0\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, max_iter=1000, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Находит вектор весов с использованием градиентного спуска по мини-батчам.\n",
    "\n",
    "    Параметры:\n",
    "    - X: Матрица признаков.\n",
    "    - y: Вектор целевых значений.\n",
    "    - learning_rate: Скорость обучения.\n",
    "    - batch_size: Размер мини-батча.\n",
    "    - max_iter: Максимальное количество итераций.\n",
    "    - tol: Критерий остановки по изменению весов.\n",
    "\n",
    "    Возвращает:\n",
    "    - Вектор весов и количество итераций.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    prev_weights = np.zeros(n)\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < max_iter and np.linalg.norm(weights - prev_weights) > tol:\n",
    "        prev_weights = weights.copy()\n",
    "        indices = np.random.choice(m, batch_size, replace=False)\n",
    "        X_batch = X[indices, :]\n",
    "        y_batch = y[indices]\n",
    "        gradient = -2/batch_size * X_batch.T @ (y_batch - X_batch @ weights)\n",
    "        weights -= learning_rate * gradient\n",
    "        iterations += 1\n",
    "\n",
    "    return weights, iterations\n",
    "\n",
    "# Загрузка данных из файла\n",
    "train_data = np.loadtxt(\"ml_lab1_train.txt\", delimiter=',')\n",
    "X_train = train_data[:, 0].reshape(-1, 1)\n",
    "y_train = train_data[:, 1]\n",
    "\n",
    "# Добавляем столбец из единиц для учета свободного члена\n",
    "X_train_with_bias = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "\n",
    "# Разные значения скорости обучения\n",
    "learning_rates = [0.5, 1.0, 2.0]\n",
    "\n",
    "# Протестируем для каждой скорости обучения\n",
    "for learning_rate in learning_rates:\n",
    "    weights, iterations = mini_batch_gradient_descent(X_train_with_bias, y_train, learning_rate=learning_rate)\n",
    "    print(f\"\\nСкорость обучения (η): {learning_rate}\")\n",
    "    print(\"Веса (градиентный спуск по мини-батчам):\", weights)\n",
    "    print(\"Количество шагов:\", iterations)\n",
    "\n",
    "    # Рисуем график исходных данных и линии регрессии\n",
    "    plt.scatter(X_train, y_train, label='Исходные данные')\n",
    "    x_line = np.linspace(np.min(X_train), np.max(X_train), 100).reshape(-1, 1)\n",
    "    X_line_with_bias = np.concatenate((np.ones((x_line.shape[0], 1)), x_line), axis=1)\n",
    "\n",
    "    y_line_mini_batch_gradient_descent = X_line_with_bias @ weights\n",
    "    plt.plot(x_line, y_line_mini_batch_gradient_descent, label=f'Линия регрессии (η={learning_rate})', linestyle='dashed')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
